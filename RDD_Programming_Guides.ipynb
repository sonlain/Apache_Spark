{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "135d7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c5ecdf",
   "metadata": {},
   "source": [
    "### Overview\n",
    "- Driver Program : Users Main function\n",
    "- Executors : Runs the job\n",
    "- Resilient Distributed Dataset : RDD, collection of elelments partition across the nodes of cluster that can be operated on parallel.\n",
    "- Variables : Broadcast Variables(all nodes), Accumulators (added)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9aec43",
   "metadata": {},
   "source": [
    "### Linking with Spark\n",
    "- pyspark 4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0852845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4607ed5b",
   "metadata": {},
   "source": [
    "### Initialising Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c029947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/14 22:28:12 WARN Utils: Your hostname, Nitishs-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.5 instead (on interface en0)\n",
      "25/07/14 22:28:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/14 22:28:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#conf=SparkConf().setAppName(\"Spark_RDD\").setMaster(\"local\")\n",
    "#sc=SparkContext(conf=conf)\n",
    "conf=SparkConf().setAppName(\"Spark_RDD\").setMaster(\"local\")\n",
    "sc=SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b156ec9",
   "metadata": {},
   "source": [
    "### Resilient Distributed Datasets (RDDs)\n",
    "- Fault Tolerant collection of elements\n",
    "- Parallel Operations\n",
    "- Parallelizing, Referencing an External Storage system that support Hadoop Input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d1c9c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=list(range(1,11))\n",
    "distributeddata=sc.parallelize(data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "935408c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 'a'),\n",
       " (2, 'aa'),\n",
       " (3, 'aaa'),\n",
       " (4, 'aaaa'),\n",
       " (5, 'aaaaa'),\n",
       " (6, 'aaaaaa'),\n",
       " (7, 'aaaaaaa'),\n",
       " (8, 'aaaaaaaa'),\n",
       " (9, 'aaaaaaaaa'),\n",
       " (10, 'aaaaaaaaaa')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=distributeddata.map(lambda x: (x,\"a\"*x))\n",
    "res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c2bc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.saveAsSequenceFile(\"/Users/nitish/Desktop/Code for Data/Py Spark/Apache Spark/Data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16382315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'),\n",
       " (2, 'aa'),\n",
       " (3, 'aaa'),\n",
       " (4, 'aaaa'),\n",
       " (5, 'aaaaa'),\n",
       " (6, 'aaaaaa'),\n",
       " (7, 'aaaaaaa'),\n",
       " (8, 'aaaaaaaa'),\n",
       " (9, 'aaaaaaaaa'),\n",
       " (10, 'aaaaaaaaaa')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sc.sequenceFile(\"/Users/nitish/Desktop/Code for Data/Py Spark/Apache Spark/Data/\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56b487ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'a')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "234a7602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (2, 'aa'), (3, 'aaa'), (4, 'aaaa')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e888f4",
   "metadata": {},
   "source": [
    "### RDD Operations\n",
    "- Transformations : Create a new dataset from existing one . Map\n",
    "- Actions : Returns a value form a driver program after running computation on a dataset . Reduce\n",
    "- Lazy Evaluation : Only computed when actions requires a result\n",
    "- Persist : Cache for faster access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6db5227e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (2, 'aa')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9df3c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2=res.map(lambda x: (x[0],x[1],len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e75fcaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a', 1),\n",
       " (2, 'aa', 2),\n",
       " (3, 'aaa', 3),\n",
       " (4, 'aaaa', 4),\n",
       " (5, 'aaaaa', 5),\n",
       " (6, 'aaaaaa', 6),\n",
       " (7, 'aaaaaaa', 7),\n",
       " (8, 'aaaaaaaa', 8),\n",
       " (9, 'aaaaaaaaa', 9),\n",
       " (10, 'aaaaaaaaaa', 10)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef5cd9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "totallength=r2.reduce(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cca621db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(totallength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffcfa828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[11] at collect at /var/folders/kv/96360ndn5zxd9bhcmb69nzlm0000gn/T/ipykernel_5452/4136562622.py:1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd2039",
   "metadata": {},
   "source": [
    "### Passing Function to Spark\n",
    "- Lambda Expression\n",
    "- Local def\n",
    "- Top Level functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ef87d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do(self,rdd):\n",
    "    field=self.field\n",
    "    return rdd.map(lambda x: field +x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06d41d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Understanding Closure : Scope and Lifecucle of a Variables and methods when executing code across a cluster\n",
    "new_rdd=sc.parallelize(data)\n",
    "new_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "724c2b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter=0\n",
    "def inc_counter(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "new_rdd.foreach(inc_counter)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45a15e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Accumulator : Mechanism for self updating variable when execution is split up across worker nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80d9a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=sc.parallelize(list(range(1,11)))\n",
    "pairs=lines.map(lambda x: (x,1))\n",
    "counts=pairs.reduceByKey(lambda a,b:a+b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a49d383f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0520cd6",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "- map(func)\n",
    "- filter(func)\n",
    "- flatMap(func)\n",
    "- mapPartitions(func)\n",
    "- mapPartitionsWithIndex(func)\n",
    "- union(other Dataset)\n",
    "- intersection (other Dataset)\n",
    "- distinct([numPartitions])\n",
    "- groupByKey([numPartitions])\n",
    "- reduceByKey(func,[numPartitions])\n",
    "- aggregateByKey(zeroValue)\n",
    "- sortBykey\n",
    "- join\n",
    "- cogroup\n",
    "- cartesian\n",
    "- pipe\n",
    "- coalesce(numPartitions)\n",
    "- repartitionAndSortWithinPartition(partitioner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb05ee37",
   "metadata": {},
   "source": [
    "#### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "054eb0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[1, 4, 9, 16, 25]\n",
      "['Hi Nitish', 'Hi Manu']\n",
      "['HI NITISH', 'HI MANU']\n",
      "['Hello, Hi Nitish', 'Hello, Hi Manu']\n",
      "['Hi Nitish', 'Hi Manu']\n",
      "[1, 2]\n",
      "[('Hi Nitish', 10), ('Hi Manu', 20)]\n",
      "\n",
      "Json_Parse:\n",
      "['Nitish', 'Manu'],[20, 19]\n",
      "\n",
      "Multiple field Extraction\n",
      "[{'name': 'Nitish', 'age': 26, 'Profession': 'Engineer'}, {'name': 'Manu', 'age': 25, 'Profession': 'Doctor'}, {'name': 'Hary', 'age': 25, 'Profession': 'Engineer'}]\n",
      "\n",
      "Mathematical Functions:\n",
      "[1.0, 1.4142135623730951, 1.7320508075688772, 2.0]\n",
      "[(1, 2, 1), (2, 4, 4), (3, 6, 9), (4, 8, 16)]\n",
      "\n",
      "Conditional Transformation\n",
      "['small', 'small', 'small', 'medium', 'medium', 'medium', 'medium', 'large', 'large']\n",
      "\n",
      "Custom Function\n",
      "[{'Name': 'NITISH', 'Age': 20, 'Designation': ' 20'}, {'Name': 'MANU', 'Age': 19, 'Designation': ' 19'}, {'Name': 'RAGU ', 'Age': 20, 'Designation': ' 20'}]\n",
      "\n",
      " map(1:1) vs flatMap(1:many)\n",
      "2,5\n",
      "[['hello', 'world'], ['spark', 'rdd', 'correct']],['hello', 'world', 'spark', 'rdd', 'correct']\n"
     ]
    }
   ],
   "source": [
    "rdd=sc.parallelize([1,2,3,4,5])\n",
    "squared=rdd.map(lambda x: x*x)\n",
    "print(rdd.collect())\n",
    "print(squared.collect())\n",
    "\n",
    "words=sc.parallelize([\"Hi Nitish\",\"Hi Manu\"])\n",
    "upper=words.map(lambda x: x.upper())\n",
    "prefix=words.map(lambda x: f\"Hello, {x}\")\n",
    "print(words.collect())\n",
    "print(upper.collect())\n",
    "print(prefix.collect())\n",
    "\n",
    "pairs=sc.parallelize([(\"Hi Nitish\",1),(\"Hi Manu\",2)])\n",
    "keys=pairs.map(lambda x: x[0])\n",
    "value=pairs.map(lambda x:x[1])\n",
    "transform= pairs.map(lambda x: (x[0],x[1]*10))\n",
    "print(keys.collect())\n",
    "print(value.collect())\n",
    "print(transform.collect())\n",
    "\n",
    "print(\"\\nJson_Parse:\")\n",
    "json_string=sc.parallelize(['{\"name\":\"Nitish\",\"age\":20}','{\"name\":\"Manu\",\"age\":19}'])\n",
    "parsed=json_string.map(lambda x:json.loads(x))\n",
    "name=parsed.map(lambda x:x['name'])\n",
    "age=parsed.map(lambda x:x['age'])\n",
    "print(f\"{name.collect()},{age.collect()}\")\n",
    "\n",
    "print(\"\\nMultiple field Extraction\")\n",
    "data=sc.parallelize([\"Nitish,26,Engineer\",\"Manu,25,Doctor\",\"Hary,25,Engineer\"])\n",
    "structured=data.map(lambda x:\n",
    "                    {\"name\":x.split(',')[0].title(),\n",
    "                    \"age\":int(x.split(',')[1]),\n",
    "                    \"Profession\":x.split(',')[2]})\n",
    "print(structured.collect())\n",
    "\n",
    "print(\"\\nMathematical Functions:\")\n",
    "data=list(range(1,5))\n",
    "numbers=sc.parallelize(data)\n",
    "sqrt_value=numbers.map(lambda x: math.sqrt(x))\n",
    "tuple_value=numbers.map(lambda x: (x,x*2,x**2))\n",
    "print(sqrt_value.collect())\n",
    "print(tuple_value.collect())\n",
    "\n",
    "print(\"\\nConditional Transformation\")\n",
    "numbers=sc.parallelize(list(range(1,10)))\n",
    "decile=numbers.map(lambda x:\n",
    "                    \"small\" if x<4 else\n",
    "                    \"medium\" if x<8 else \"large\" )\n",
    "print(decile.collect())\n",
    "\n",
    "\n",
    "print(\"\\nCustom Function\")\n",
    "def process_record(data):\n",
    "    parts=data.split(\",\")\n",
    "    return {\n",
    "        \"Name\":parts[0].upper(),\n",
    "        \"Age\":int(parts[1]),\n",
    "        \"Designation\": parts[1]\n",
    "    }\n",
    "\n",
    "raw_data=sc.parallelize({\n",
    "    \"Nitish, 20, Engineer\",\"Manu, 19, Doctor\",\"Ragu , 20,Engineer\"\n",
    "})\n",
    "process_data=raw_data.map(process_record)\n",
    "print(process_data.collect())\n",
    "\n",
    "\n",
    "print(\"\\n map(1:1) vs flatMap(1:many)\")\n",
    "words=sc.parallelize([\"hello world\",\"spark rdd correct\"])\n",
    "val1=words.map(lambda x: x.split())\n",
    "val2=words.flatMap(lambda x: x.split())\n",
    "print(f'{val1.count()},{val2.count()}')\n",
    "print(f'{val1.collect()},{val2.collect()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384eb740",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e547d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n",
      "[6, 7, 8, 9, 10]\n",
      "\n",
      "Filter Strings\n",
      "['banana', 'litchi', 'strawberry']\n",
      "\n",
      "Range and Boundary Filtering\n",
      "[99, 85, 84, 92]\n",
      "[99, 85, 92]\n",
      "\n",
      " Text and Pattern filtering \n",
      "['The quick brown fox', 'Jumps over the lazy dog']\n",
      "['nitishtripurawork@gmail.com', 'testemail123@gmail.in', 'mywork@gmail.in', 'staf@edu.in']\n",
      "\n",
      " Key value Filtering\n",
      "[('cherry', 3)]\n",
      "[('cherry', 3), ('muskmelon', 3)]\n",
      "\n",
      " Complex filtering with multiple conditions\n",
      "[('Mark', 95, 'Math')]\n",
      "[('Bob', 65, 'CSE'), ('Mark', 95, 'Math'), ('Methew', 86, 'Math')]\n",
      "\n",
      " Custom Business Logic\n",
      "[{'name': 'John', 'purchases': 8, 'total_spent': 1500, 'membership_years': 3}, {'name': 'Bob', 'purchases': 12, 'total_spent': 2000, 'membership_years': 4}]\n",
      "\n",
      " Combining Filter with map\n",
      "[144, 196, 256, 324, 400, 484, 576, 676, 784, 900, 1024, 1156, 1296, 1444, 1600, 1764, 1936, 2116, 2304, 2500, 2704, 2916, 3136, 3364, 3600, 3844, 4096, 4356, 4624, 4900, 5184, 5476, 5776, 6084, 6400, 6724, 7056, 7396, 7744, 8100, 8464, 8836, 9216, 9604]\n"
     ]
    }
   ],
   "source": [
    "numbers=sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "evens=numbers.filter(lambda x: x%2==0)\n",
    "greater_than_5=numbers.filter(lambda x :x>5)\n",
    "print(evens.collect())\n",
    "print(greater_than_5.collect())\n",
    "\n",
    "print(\"\\nFilter Strings\")\n",
    "strings=sc.parallelize([\"apple\",\"banana\",\"litchi\",\"kiwi\",\"strawberry\"])\n",
    "len_5=strings.filter(lambda x: len(x)>5)\n",
    "print(len_5.collect())\n",
    "\n",
    "print(\"\\nRange and Boundary Filtering\")\n",
    "scores=sc.parallelize([60,69,74,99,85,84,92])\n",
    "passing_scores=scores.filter(lambda x: x>=75)\n",
    "excellent=scores.filter(lambda x: 85 <= x <= 100)\n",
    "print(passing_scores.collect())\n",
    "print(excellent.collect())\n",
    "\n",
    "print(\"\\n Text and Pattern filtering \")\n",
    "sentence=sc.parallelize([\"The quick brown fox\",\"Jumps over the lazy dog\",\"Python is awsome\",\"Spark Processes big Data\"])\n",
    "contains_the=sentence.filter(lambda x: \"the\" in x.lower())\n",
    "email_pattern=r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9._]+[a-zA-Z]{2,}$'\n",
    "emails=sc.parallelize([\"nitishtripurawork@gmail.com\",\"testemail123@gmail.in\",\"mywork@gmail.in\",\"staf@edu.in\",\"123@ae@#.incom\"])\n",
    "valid_emails=emails.filter(lambda x: re.match(email_pattern,x))\n",
    "print(contains_the.collect())\n",
    "print(valid_emails.collect())\n",
    "\n",
    "\n",
    "print(\"\\n Key value Filtering\")\n",
    "key_value_pairs=sc.parallelize([(\"apple\",1),(\"banana\",2),(\"cherry\",3),(\"date\",2),(\"muskmelon\",3)])\n",
    "starts_with_c=key_value_pairs.filter(lambda x: x[0].startswith('c'))\n",
    "val_greater_than_2=key_value_pairs.filter(lambda x:x[1]>2)\n",
    "print(starts_with_c.collect())\n",
    "print(val_greater_than_2.collect())\n",
    "\n",
    "print(\"\\n Complex filtering with multiple conditions\")\n",
    "students=sc.parallelize([(\"Alice\", 85, \"Math\"),(\"Bob\",65,\"CSE\"),(\"Mark\",95,\"Math\"),(\"Methew\",86,\"Math\")])\n",
    "math_high_score=students.filter(lambda x: x[1]>90 and x[2].lower()==\"math\")\n",
    "M_or_CSE=students.filter(lambda x: x[0][0]==\"M\" or x[2]==\"CSE\")\n",
    "print(math_high_score.collect())\n",
    "print(M_or_CSE.collect())\n",
    "\n",
    "print(\"\\n Custom Business Logic\")\n",
    "def is_premium(data):\n",
    "    return (data[\"purchases\"]>5 and data[\"total_spent\"]>1000 and data[\"membership_years\"]>=2)\n",
    "cus=sc.parallelize([{\"name\":\"John\",\"purchases\":8,\"total_spent\":1500,\"membership_years\":3},\n",
    "                    {\"name\":\"Jane\",\"purchases\":3,\"total_spent\":800,\"membership_years\":1},\n",
    "                    {\"name\":\"Bob\",\"purchases\":12,\"total_spent\":2000,\"membership_years\":4}])\n",
    "premium_customers=cus.filter(is_premium)\n",
    "print(premium_customers.collect())\n",
    "\n",
    "print(\"\\n Combining Filter with map\")\n",
    "numbers=sc.parallelize(range(1,100))\n",
    "result=numbers.filter(lambda x:x%2==0).map(lambda x:x*x).filter(lambda x:x>100).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381cf792",
   "metadata": {},
   "source": [
    "### Actions\n",
    "- reduce(func)\n",
    "- collect()\n",
    "- count()\n",
    "- first()\n",
    "- take(n)\n",
    "- takeOrdered(n,[ordering])\n",
    "- saveAsTextFile(path)\n",
    "- savsAsSequenceFile(path)\n",
    "- countByKey()\n",
    "- foreach(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "482b65de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa250d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
